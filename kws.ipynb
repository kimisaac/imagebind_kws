{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae11633-63a6-4807-b880-523f7be4821d",
   "metadata": {},
   "source": [
    "Install libraries as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8094fbab-add6-4e34-8c24-1762c21f66d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sounddevice in /home/kim_isaac_buelagala/.local/lib/python3.8/site-packages (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.8/dist-packages (from sounddevice) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from CFFI>=1.0->sounddevice) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice\n",
    "!pip install soundfile\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85a082",
   "metadata": {},
   "source": [
    "Clone Imagebind to current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd1fe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in C:/Users/Dell/Downloads/COE/197Z/p2/.git/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/facebookresearch/ImageBind\n",
      " * branch            main       -> FETCH_HEAD\n",
      " * [new branch]      main       -> origin/main\n"
     ]
    }
   ],
   "source": [
    "!git init\n",
    "!git remote add origin https://github.com/facebookresearch/ImageBind.git\n",
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03697429",
   "metadata": {},
   "source": [
    "Download SPEECHCOMMANDS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd974f09-99b0-4861-b287-b2ab43183f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.43G/2.43G [02:13<00:00, 18.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved to ./speech_commands_v0.02.tar.gz\n",
      "Extracting gz...\n",
      "File extracted and saved to ./speech_commands_v0.02.tar\n",
      "Extracting tar...\n",
      "File extracted and saved to ./speech_commands_v0.02\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "file_path = \"./speech_commands_v0.02.tar.gz\"\n",
    "if not os.path.exists(file_path):\n",
    "    url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "    folder_path = './'  # Replace with the desired folder path\n",
    "\n",
    "    file_name = url.split('/')[-1]  # Extract the file name from the URL\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 KB\n",
    "    progress_bar = tqdm(total=total_size, unit='B', unit_scale=True)\n",
    "\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    print(f\"File downloaded and saved to {file_path}\")\n",
    "if not os.path.exists('./speech_commands_v0.02.tar'):\n",
    "    print(\"Extracting gz...\")\n",
    "    file_path = './speech_commands_v0.02.tar.gz'  # Replace with the path to your .gz file\n",
    "    folder_path = './'  # Replace with the desired folder path\n",
    "\n",
    "    # Extract the file name from the path\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    # Construct the output file path by removing the .gz extension\n",
    "    output_file_path = os.path.join(folder_path, file_name[:-3])\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as gz_file:\n",
    "        with open(output_file_path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(gz_file, out_file)\n",
    "\n",
    "    print(f\"File extracted and saved to {output_file_path}\")\n",
    "\n",
    "if not os.path.exists('./speech_commands_v0.02/'):\n",
    "    print(\"Extracting tar...\")\n",
    "    file_path = './speech_commands_v0.02.tar'  # Replace with the path to your .tar file\n",
    "    folder_path = './'  # Replace with the desired folder path\n",
    "\n",
    "    # Extract the file name from the path\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    # Construct the output folder path by removing the .tar extension\n",
    "    output_folder_path = os.path.join(folder_path, file_name[:-4])\n",
    "\n",
    "    with tarfile.open(file_path, 'r') as tar:\n",
    "        tar.extractall(output_folder_path)\n",
    "\n",
    "    print(f\"File extracted and saved to {output_folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b7d3f",
   "metadata": {},
   "source": [
    "Import Modules and Declare Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3c2f153-20c7-4b51-ac5e-50125f2f2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import data\n",
    "import torch, torchaudio\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "\n",
    "def record_to_wav(filename, duration, sample_rate):\n",
    "    # Set up the recording parameters\n",
    "    sd.default.samplerate = sample_rate\n",
    "    sd.default.channels = 1  # Mono recording\n",
    "\n",
    "    # Start the recording\n",
    "    print(\"Recording started. Speak into the microphone now!\")\n",
    "    audio = sd.rec(int(duration * sample_rate), dtype='float32')\n",
    "    \n",
    "    # Wait for the recording to complete\n",
    "    sd.wait()\n",
    "\n",
    "    # Save the recorded audio to a WAV file using soundfile\n",
    "    sf.write(filename, audio, sample_rate)\n",
    "\n",
    "    print(f\"Recording saved as {filename}.\")\n",
    "    return filename\n",
    "\n",
    "def pad_wav(input_path, duration=5):\n",
    "    output_path = \"./temp_sound.wav\"\n",
    "    # Load the waveform from the input WAV file\n",
    "    waveform, sample_rate = torchaudio.load(input_path)\n",
    "    \n",
    "    target_samples = int(duration * sample_rate)\n",
    "    waveform_samples = waveform.size(1)\n",
    "    if waveform_samples > target_samples:\n",
    "        return input_path\n",
    "    waveform = waveform.view(-1)\n",
    "\n",
    "    # Calculate the number of samples for the padding\n",
    "    padding_samples = (target_samples - waveform_samples) // 2\n",
    "\n",
    "    # Create a tensor of zeros for the padding\n",
    "    padding = torch.zeros(padding_samples)\n",
    "\n",
    "    # Pad the waveform with zeros before and after\n",
    "    padded_waveform = torch.cat((padding, waveform, padding))\n",
    "\n",
    "    # Reshape the padded waveform to a 2-dimensional tensor\n",
    "    padded_waveform = padded_waveform.view(1, -1)\n",
    "\n",
    "    # Save the padded waveform to a new WAV file\n",
    "    torchaudio.save(output_path, padded_waveform, sample_rate)\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5323c72",
   "metadata": {},
   "source": [
    "Read Testing list file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c35472-077b-430f-b2fa-cb7321ed839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './speech_commands_v0.02/testing_list.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    test_list = [line.rstrip('\\n') for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afeaa8",
   "metadata": {},
   "source": [
    "Pick if sample is to be recorded or choses from test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a829868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started. Speak into the microphone now!\n",
      "Recording saved as ./temp_sound.wav.\n"
     ]
    }
   ],
   "source": [
    "choice = input(\"Enter 0 if you want to record your own voice, 1 if you want to use a sample from test split: \")\n",
    "if choice:\n",
    "    sample = record_to_wav(\"./temp_sound.wav\", 5, 16000)\n",
    "    correct_class = \"recorded, not in test split\"\n",
    "else:\n",
    "    rand_sample = np.random.randint(len(test_list))\n",
    "    sample = \"./speech_commands_v0.02/\"+test_list[rand_sample]\n",
    "    print(sample)\n",
    "    correct_class = sample.split(\"/\")[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd627f",
   "metadata": {},
   "source": [
    "Demo: Classify sound sample and show correct class if from test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c575355-4e7d-4396-881a-b17e4aa5e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow', 'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
    "\n",
    "wav = pad_wav(sample)\n",
    "text_list = CLASSES\n",
    "\n",
    "audio_paths=[wav]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    #ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "results = torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1)\n",
    "res_list = results.tolist()[0]\n",
    "print(\"Imagebind Inference: \",CLASSES[res_list.index(max(res_list))])\n",
    "print(\"Ground Truth: \", correct_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458cb00",
   "metadata": {},
   "source": [
    "Evaluating the model by running all or part of the test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19000bc5-f789-468e-81b7-abd1b53b511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percentage = 100 # Change this to test on a different percentage of the test split takes velues from 0 to 100\n",
    "test_list = test_list[:int(len(test_list)*test_percentage/100)]\n",
    "\n",
    "print_flag = False\n",
    "score = 0\n",
    "for sound in test_list:\n",
    "    audio_paths = [pad_wav(sound)]\n",
    "    correct_class = sound.split(\"/\")[2]\n",
    "    \n",
    "    inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)\n",
    "        \n",
    "    results = torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1)\n",
    "    res_list = results.tolist()[0]\n",
    "    if print_flag:\n",
    "        print(\"Imagebind Inference: \",CLASSES[res_list.index(max(res_list))])\n",
    "        print(\"Ground Truth: \", correct_class)\n",
    "    if (correct_class == CLASSES[res_list.index(max(res_list))]):\n",
    "        score += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d9966-5c06-4b17-9f9d-c8f629253a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = score/len(test_list)\n",
    "print(\"Imagebind Zero shot\" )\n",
    "print(\"Number of test samples: \", len(test_list))\n",
    "print(\" Accuracy : \", accuracy )\n",
    "print(\"TripLet Loss res 15 accuracy\")\n",
    "print(\"SOTA Accuracy : 0.9856\") #TripletLoss-res15: https://github.com/roman-vygon/triplet_loss_kws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SOTA Models Scores\")\n",
    "print(\"| Model                 | Training Approach   | Evaluation     | Accuracy   |\")\n",
    "print(\"|-----------------------|---------------------|----------------|------------|\")\n",
    "print(\"| Imagebind             | Unsupervised        | Zero Shot      | {:.4f}     |\".format(accuracy))\n",
    "print(\"| TripletLoss-res15     | Unsupervised        | Not zero or few| {:.4f}     |\".format(0.9856))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e35676d33e03a9093a17dc7760f2b2aa4960ff6d3c0d7dc06348afdc0ca22c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
